{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq任务构建训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 打开文件\n",
    "2. 返回字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir='./../data/fra-eng'\n",
    "with open(os.path.join(data_dir, 'fra.txt'), 'r',encoding='utf-8') as f:\n",
    "    raw_text= f.read()\n",
    "#字符串\n",
    "print(type(raw_text))\n",
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对文本的预处理\n",
    "1. 特殊空格符号，统一空格\n",
    "2. 统一小写\n",
    "3. 单词和标点符号之间加上空格\n",
    "4. 确定token级别是单词，不同token之间用空格分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who ?\tqui ?\n",
      "wow !\tça alors !\n"
     ]
    }
   ],
   "source": [
    "# text是文本字符串\n",
    "def preprocess_nmt(text):\n",
    "    \"\"\"预处理“英语－法语”数据集\"\"\"\n",
    "\n",
    "    # char是字符，prev_char是前一个字符\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # 使用空格替换不间断空格\n",
    "    # 使用小写字母替换大写字母\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "\n",
    "    # 在单词和标点符号之间插入空格\n",
    "    #遍历字符串\n",
    "    #如果当前字符是标点符号，并且上一个字符不是空格。那么在当前字符前面加空格\n",
    "    #其他情况一律保留当前字符\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(type(text))\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本字符串分割+token分割\n",
    "- 输入text：文本字符串\n",
    "- 输出：\n",
    "  1. 源语言token为粒度的，序列\n",
    "  2. 源语言token为粒度的，序列\n",
    "- source：两层list, `[[token_1,...,token_n],...,[token_1,...,token_n]]`\n",
    "- target：两层list, `[[token_1,...,token_n],...,[token_1,...,token_n]]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['go', '.'],\n",
       "  ['hi', '.'],\n",
       "  ['run', '!'],\n",
       "  ['run', '!'],\n",
       "  ['who', '?'],\n",
       "  ['wow', '!']],\n",
       " [['va', '!'],\n",
       "  ['salut', '!'],\n",
       "  ['cours', '!'],\n",
       "  ['courez', '!'],\n",
       "  ['qui', '?'],\n",
       "  ['ça', 'alors', '!']])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text：文本字符串\n",
    "def tokenize_nmt(text, num_examples=None):\n",
    "    #分割依据是：换行符、制表符、空格\n",
    "    #换行符：不同序列\n",
    "    #制表符：分割源语言和目标语言\n",
    "    #空格：分割不同token\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "#source, target都是双层list，里面一层的list是不定长度的序列\n",
    "#token是单词，包括标点符号\n",
    "source_tokens, target_tokens = tokenize_nmt(text)\n",
    "source_tokens[:6], target_tokens[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[159,  13, 108,   6, 828,   9,   3,   1,   1,   1],\n",
      "        [  5,  36,  45,  35, 357,   7,  25, 294,  23, 113]], dtype=torch.int32)\n",
      "X的有效长度: tensor([ 7, 10])\n",
      "Y: tensor([[  58,   41, 4326,  922,    7,    3,    1,    1,    1,    1],\n",
      "        [   5,   78,    9,   22,   11,  344,    8,   10,   65,  293]],\n",
      "       dtype=torch.int32)\n",
      "Y的有效长度: tensor([ 6, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from vocabulary import Vocab\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self,tokens,num_steps=10,min_freq=2,reserved_tokens=['<pad>', '<bos>', '<eos>']):\n",
    "        self.tokens = tokens\n",
    "        self.vocab = Vocab(self.tokens, min_freq=min_freq,reserved_tokens =reserved_tokens)\n",
    "        self.truncate_pad_tensor, self.valid_len, = self.truncate_pad(num_steps)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.truncate_pad_tensor.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return (self.truncate_pad_tensor[index], self.valid_len[index])\n",
    "    \n",
    "    \n",
    "    def truncate_pad(self,num_steps,ending_token='<eos>', padding_token='<pad>'):\n",
    "        sequences_idx = [self.vocab[sequence] for sequence in self.tokens]\n",
    "        #每个序列后面加一个'<eos>'索引\n",
    "        sequences_idx = [idx + [self.vocab[ending_token]] for idx in sequences_idx]\n",
    "        # 每个序列截断填充处理\n",
    "        truncate_pad=(\n",
    "            [sequence_idx[:num_steps] if len(sequence_idx)>num_steps \n",
    "            else sequence_idx + [self.vocab[padding_token]] * (num_steps - len(sequence_idx)) \n",
    "            for sequence_idx in sequences_idx])\n",
    "\n",
    "        # 形状：(s,num_steps)\n",
    "        # 其中s是序列数目，num_steps是固定的序列长度\n",
    "        truncate_pad_tensor = torch.tensor(truncate_pad)\n",
    "        # 有效长度\n",
    "        valid_len = (truncate_pad_tensor != self.vocab['<pad>']).type(torch.int32).sum(1)\n",
    "        return (truncate_pad_tensor, valid_len)\n",
    "\n",
    "class Seq2seqDataset(Dataset):\n",
    "    def __init__(self,source_dataset,target_dataset):\n",
    "        self.source_dataset = source_dataset\n",
    "        self.target_dataset = target_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.source_dataset)==len(self.target_dataset)\n",
    "        return (len(self.source_dataset))\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return (self.source_dataset[index][0],self.source_dataset[index][1],\n",
    "        self.target_dataset[index][0],self.target_dataset[index][1])\n",
    "\n",
    "source_dataset = SequenceDataset(source_tokens)\n",
    "target_dataset = SequenceDataset(target_tokens)\n",
    "\n",
    "seq2seq_dataset = Seq2seqDataset(source_dataset,target_dataset)\n",
    "seq2seq_dataset[0]\n",
    "\n",
    "data_iter= DataLoader(seq2seq_dataset, batch_size=2, shuffle=True)\n",
    "for X, X_valid_len, Y, Y_valid_len in data_iter:\n",
    "    print('X:', X.type(torch.int32))\n",
    "    print('X的有效长度:', X_valid_len)\n",
    "    print('Y:', Y.type(torch.int32))\n",
    "    print('Y的有效长度:', Y_valid_len)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
