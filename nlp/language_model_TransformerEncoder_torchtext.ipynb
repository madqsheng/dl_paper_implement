{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 nn.TransformerEncoder 和 torchtext 构建语言模型训练（单词级别）\n",
    "1. 不涉及nn.TransformerDecoder\n",
    "2. nn.TransformerEncoder 由多层nn.TransformerEncoderLayer组成\n",
    "3. 对于语言模型任务，未来位置上的任何标记都应该被屏蔽。确保位置 i 的预测只能依赖于小于 i 的位置处的已知输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        \"\"\"初始化位置编码矩阵：\n",
    "        pe形状:(max_len, d_model)\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d_model : int\n",
    "            位置编码维度\n",
    "        dropout : float, optional\n",
    "            失活比例, by default 0.1\n",
    "        max_len : int, optional\n",
    "            位置编码最大长度,它会根据输入X的长度截断 by default 5000\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        \"\"\"以nn.TransformerEncoder为核心构建模型\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ntoken : int\n",
    "            词表中token数量\n",
    "        d_model : int\n",
    "            token嵌入维度\n",
    "        nhead : int\n",
    "            多头注意力中头数量\n",
    "        d_hid : int\n",
    "            隐藏层味道\n",
    "        nlayers : int\n",
    "            transform块数量\n",
    "        dropout : float, optional\n",
    "            dropout比例, by default 0.5\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        # TransformerEncoderLayer块\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据\n",
    "batchify函数\n",
    "\n",
    "![](./../示例图片/划分批量.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "#一行行的数据文本数据\n",
    "train_iter = WikiText2(split='train')\n",
    "#分词工具，单词级别\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "\n",
    "#tensor,1d,索引值\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#给定一个连续数据的一维向量划分(full_seq_len, batch_size)向量\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "# shape `[full_seq_len, batch_size]`\n",
    "train_data = batchify(train_data, batch_size)  \n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 22.57 | loss  8.16 | ppl  3506.77\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch  6.04 | loss  6.86 | ppl   957.79\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch  5.87 | loss  6.43 | ppl   619.97\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch  5.93 | loss  6.28 | ppl   536.28\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch  5.96 | loss  6.19 | ppl   487.45\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch  5.97 | loss  6.15 | ppl   469.09\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch  5.95 | loss  6.11 | ppl   449.92\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch  5.91 | loss  6.11 | ppl   449.38\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch  5.94 | loss  6.03 | ppl   413.88\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch  5.91 | loss  6.02 | ppl   410.30\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch  6.04 | loss  5.90 | ppl   365.07\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch  6.05 | loss  5.97 | ppl   392.06\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch  5.92 | loss  5.95 | ppl   384.38\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch  6.02 | loss  5.88 | ppl   356.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 21.78s | valid loss  5.79 | valid ppl   326.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch  5.94 | loss  5.87 | ppl   353.45\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch  5.87 | loss  5.85 | ppl   346.92\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch  5.87 | loss  5.66 | ppl   287.94\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch  5.87 | loss  5.70 | ppl   297.67\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch  5.91 | loss  5.65 | ppl   284.84\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch  6.00 | loss  5.68 | ppl   291.94\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch  5.97 | loss  5.69 | ppl   295.17\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch  6.02 | loss  5.71 | ppl   303.28\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch  5.96 | loss  5.65 | ppl   285.09\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch  5.89 | loss  5.67 | ppl   290.64\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch  6.15 | loss  5.55 | ppl   257.38\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch  5.94 | loss  5.65 | ppl   283.87\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch  5.91 | loss  5.65 | ppl   284.69\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch  5.95 | loss  5.59 | ppl   266.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 18.42s | valid loss  5.70 | valid ppl   298.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch  6.12 | loss  5.61 | ppl   272.80\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch  5.96 | loss  5.63 | ppl   278.32\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch  5.96 | loss  5.43 | ppl   227.78\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch  5.94 | loss  5.48 | ppl   241.00\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch  5.93 | loss  5.43 | ppl   228.81\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch  5.92 | loss  5.47 | ppl   237.38\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch  5.93 | loss  5.50 | ppl   244.15\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch  5.90 | loss  5.53 | ppl   251.21\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch  5.88 | loss  5.47 | ppl   237.86\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch  5.86 | loss  5.48 | ppl   239.65\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch  5.85 | loss  5.36 | ppl   211.98\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch  5.91 | loss  5.46 | ppl   235.12\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch  5.95 | loss  5.47 | ppl   237.89\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch  5.97 | loss  5.40 | ppl   221.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 18.37s | valid loss  5.61 | valid ppl   271.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2928 batches | lr 4.29 | ms/batch  5.94 | loss  5.43 | ppl   227.24\n",
      "| epoch   4 |   400/ 2928 batches | lr 4.29 | ms/batch  5.93 | loss  5.45 | ppl   233.47\n",
      "| epoch   4 |   600/ 2928 batches | lr 4.29 | ms/batch  5.98 | loss  5.26 | ppl   192.75\n",
      "| epoch   4 |   800/ 2928 batches | lr 4.29 | ms/batch  5.93 | loss  5.33 | ppl   205.68\n",
      "| epoch   4 |  1000/ 2928 batches | lr 4.29 | ms/batch  5.93 | loss  5.29 | ppl   198.09\n",
      "| epoch   4 |  1200/ 2928 batches | lr 4.29 | ms/batch  5.88 | loss  5.33 | ppl   206.07\n",
      "| epoch   4 |  1400/ 2928 batches | lr 4.29 | ms/batch  5.99 | loss  5.35 | ppl   211.06\n",
      "| epoch   4 |  1600/ 2928 batches | lr 4.29 | ms/batch  5.99 | loss  5.39 | ppl   218.41\n",
      "| epoch   4 |  1800/ 2928 batches | lr 4.29 | ms/batch  5.87 | loss  5.34 | ppl   209.38\n",
      "| epoch   4 |  2000/ 2928 batches | lr 4.29 | ms/batch  5.93 | loss  5.36 | ppl   211.90\n",
      "| epoch   4 |  2200/ 2928 batches | lr 4.29 | ms/batch  5.91 | loss  5.22 | ppl   185.14\n",
      "| epoch   4 |  2400/ 2928 batches | lr 4.29 | ms/batch  6.01 | loss  5.34 | ppl   207.53\n",
      "| epoch   4 |  2600/ 2928 batches | lr 4.29 | ms/batch  6.16 | loss  5.34 | ppl   209.38\n",
      "| epoch   4 |  2800/ 2928 batches | lr 4.29 | ms/batch  5.92 | loss  5.27 | ppl   194.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 18.37s | valid loss  5.54 | valid ppl   255.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2928 batches | lr 4.07 | ms/batch  5.84 | loss  5.31 | ppl   203.06\n",
      "| epoch   5 |   400/ 2928 batches | lr 4.07 | ms/batch  5.88 | loss  5.34 | ppl   208.16\n",
      "| epoch   5 |   600/ 2928 batches | lr 4.07 | ms/batch  5.97 | loss  5.15 | ppl   172.57\n",
      "| epoch   5 |   800/ 2928 batches | lr 4.07 | ms/batch  6.05 | loss  5.20 | ppl   181.90\n",
      "| epoch   5 |  1000/ 2928 batches | lr 4.07 | ms/batch  5.88 | loss  5.16 | ppl   174.52\n",
      "| epoch   5 |  1200/ 2928 batches | lr 4.07 | ms/batch  5.87 | loss  5.21 | ppl   183.98\n",
      "| epoch   5 |  1400/ 2928 batches | lr 4.07 | ms/batch  5.85 | loss  5.23 | ppl   187.73\n",
      "| epoch   5 |  1600/ 2928 batches | lr 4.07 | ms/batch  5.84 | loss  5.27 | ppl   195.00\n",
      "| epoch   5 |  1800/ 2928 batches | lr 4.07 | ms/batch  5.88 | loss  5.22 | ppl   185.64\n",
      "| epoch   5 |  2000/ 2928 batches | lr 4.07 | ms/batch  5.90 | loss  5.22 | ppl   185.65\n",
      "| epoch   5 |  2200/ 2928 batches | lr 4.07 | ms/batch  5.92 | loss  5.09 | ppl   163.02\n",
      "| epoch   5 |  2400/ 2928 batches | lr 4.07 | ms/batch  5.94 | loss  5.23 | ppl   186.55\n",
      "| epoch   5 |  2600/ 2928 batches | lr 4.07 | ms/batch  5.96 | loss  5.23 | ppl   187.37\n",
      "| epoch   5 |  2800/ 2928 batches | lr 4.07 | ms/batch  5.94 | loss  5.16 | ppl   174.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 18.29s | valid loss  5.53 | valid ppl   251.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2928 batches | lr 3.87 | ms/batch  5.92 | loss  5.20 | ppl   181.91\n",
      "| epoch   6 |   400/ 2928 batches | lr 3.87 | ms/batch  5.96 | loss  5.23 | ppl   186.81\n",
      "| epoch   6 |   600/ 2928 batches | lr 3.87 | ms/batch  5.96 | loss  5.03 | ppl   153.66\n",
      "| epoch   6 |   800/ 2928 batches | lr 3.87 | ms/batch  5.96 | loss  5.10 | ppl   163.42\n",
      "| epoch   6 |  1000/ 2928 batches | lr 3.87 | ms/batch  5.97 | loss  5.07 | ppl   159.21\n",
      "| epoch   6 |  1200/ 2928 batches | lr 3.87 | ms/batch  5.78 | loss  5.12 | ppl   166.68\n",
      "| epoch   6 |  1400/ 2928 batches | lr 3.87 | ms/batch  5.85 | loss  5.14 | ppl   170.37\n",
      "| epoch   6 |  1600/ 2928 batches | lr 3.87 | ms/batch  5.85 | loss  5.18 | ppl   176.95\n",
      "| epoch   6 |  1800/ 2928 batches | lr 3.87 | ms/batch  5.90 | loss  5.13 | ppl   169.70\n",
      "| epoch   6 |  2000/ 2928 batches | lr 3.87 | ms/batch  5.96 | loss  5.14 | ppl   170.60\n",
      "| epoch   6 |  2200/ 2928 batches | lr 3.87 | ms/batch  5.99 | loss  5.00 | ppl   147.97\n",
      "| epoch   6 |  2400/ 2928 batches | lr 3.87 | ms/batch  5.95 | loss  5.12 | ppl   167.61\n",
      "| epoch   6 |  2600/ 2928 batches | lr 3.87 | ms/batch  5.91 | loss  5.13 | ppl   169.27\n",
      "| epoch   6 |  2800/ 2928 batches | lr 3.87 | ms/batch  5.95 | loss  5.06 | ppl   157.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 18.33s | valid loss  5.58 | valid ppl   264.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2928 batches | lr 3.68 | ms/batch  5.95 | loss  5.11 | ppl   165.82\n",
      "| epoch   7 |   400/ 2928 batches | lr 3.68 | ms/batch  5.96 | loss  5.13 | ppl   168.63\n",
      "| epoch   7 |   600/ 2928 batches | lr 3.68 | ms/batch  5.95 | loss  4.95 | ppl   140.54\n",
      "| epoch   7 |   800/ 2928 batches | lr 3.68 | ms/batch  5.97 | loss  5.01 | ppl   149.69\n",
      "| epoch   7 |  1000/ 2928 batches | lr 3.68 | ms/batch  5.86 | loss  4.98 | ppl   146.02\n",
      "| epoch   7 |  1200/ 2928 batches | lr 3.68 | ms/batch  5.88 | loss  5.03 | ppl   153.31\n",
      "| epoch   7 |  1400/ 2928 batches | lr 3.68 | ms/batch  5.86 | loss  5.05 | ppl   156.33\n",
      "| epoch   7 |  1600/ 2928 batches | lr 3.68 | ms/batch  5.91 | loss  5.08 | ppl   161.14\n",
      "| epoch   7 |  1800/ 2928 batches | lr 3.68 | ms/batch  5.94 | loss  5.05 | ppl   156.00\n",
      "| epoch   7 |  2000/ 2928 batches | lr 3.68 | ms/batch  5.99 | loss  5.05 | ppl   155.59\n",
      "| epoch   7 |  2200/ 2928 batches | lr 3.68 | ms/batch  5.99 | loss  4.91 | ppl   135.81\n",
      "| epoch   7 |  2400/ 2928 batches | lr 3.68 | ms/batch  5.96 | loss  5.03 | ppl   153.67\n",
      "| epoch   7 |  2600/ 2928 batches | lr 3.68 | ms/batch  5.88 | loss  5.04 | ppl   155.14\n",
      "| epoch   7 |  2800/ 2928 batches | lr 3.68 | ms/batch  5.92 | loss  4.98 | ppl   145.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 18.32s | valid loss  5.50 | valid ppl   244.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2928 batches | lr 3.49 | ms/batch  6.07 | loss  5.02 | ppl   151.48\n",
      "| epoch   8 |   400/ 2928 batches | lr 3.49 | ms/batch  6.00 | loss  5.05 | ppl   156.38\n",
      "| epoch   8 |   600/ 2928 batches | lr 3.49 | ms/batch  5.98 | loss  4.86 | ppl   129.45\n",
      "| epoch   8 |   800/ 2928 batches | lr 3.49 | ms/batch  5.94 | loss  4.93 | ppl   139.03\n",
      "| epoch   8 |  1000/ 2928 batches | lr 3.49 | ms/batch  5.98 | loss  4.90 | ppl   134.25\n",
      "| epoch   8 |  1200/ 2928 batches | lr 3.49 | ms/batch  5.85 | loss  4.95 | ppl   141.46\n",
      "| epoch   8 |  1400/ 2928 batches | lr 3.49 | ms/batch  5.90 | loss  4.97 | ppl   144.40\n",
      "| epoch   8 |  1600/ 2928 batches | lr 3.49 | ms/batch  5.99 | loss  5.01 | ppl   149.59\n",
      "| epoch   8 |  1800/ 2928 batches | lr 3.49 | ms/batch  5.97 | loss  4.97 | ppl   144.31\n",
      "| epoch   8 |  2000/ 2928 batches | lr 3.49 | ms/batch  5.93 | loss  4.97 | ppl   143.85\n",
      "| epoch   8 |  2200/ 2928 batches | lr 3.49 | ms/batch  5.94 | loss  4.83 | ppl   125.45\n",
      "| epoch   8 |  2400/ 2928 batches | lr 3.49 | ms/batch  5.89 | loss  4.96 | ppl   142.29\n",
      "| epoch   8 |  2600/ 2928 batches | lr 3.49 | ms/batch  6.01 | loss  4.97 | ppl   144.01\n",
      "| epoch   8 |  2800/ 2928 batches | lr 3.49 | ms/batch  5.86 | loss  4.90 | ppl   134.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 18.42s | valid loss  5.52 | valid ppl   249.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2928 batches | lr 3.32 | ms/batch  5.97 | loss  4.95 | ppl   140.85\n",
      "| epoch   9 |   400/ 2928 batches | lr 3.32 | ms/batch  5.94 | loss  4.97 | ppl   143.81\n",
      "| epoch   9 |   600/ 2928 batches | lr 3.32 | ms/batch  5.93 | loss  4.80 | ppl   121.20\n",
      "| epoch   9 |   800/ 2928 batches | lr 3.32 | ms/batch  5.88 | loss  4.86 | ppl   128.91\n",
      "| epoch   9 |  1000/ 2928 batches | lr 3.32 | ms/batch  5.89 | loss  4.83 | ppl   125.55\n",
      "| epoch   9 |  1200/ 2928 batches | lr 3.32 | ms/batch  5.86 | loss  4.89 | ppl   132.34\n",
      "| epoch   9 |  1400/ 2928 batches | lr 3.32 | ms/batch  5.95 | loss  4.89 | ppl   133.45\n",
      "| epoch   9 |  1600/ 2928 batches | lr 3.32 | ms/batch  5.90 | loss  4.94 | ppl   139.09\n",
      "| epoch   9 |  1800/ 2928 batches | lr 3.32 | ms/batch  5.92 | loss  4.91 | ppl   135.16\n",
      "| epoch   9 |  2000/ 2928 batches | lr 3.32 | ms/batch  6.01 | loss  4.91 | ppl   136.02\n",
      "| epoch   9 |  2200/ 2928 batches | lr 3.32 | ms/batch  5.88 | loss  4.77 | ppl   117.97\n",
      "| epoch   9 |  2400/ 2928 batches | lr 3.32 | ms/batch  5.83 | loss  4.88 | ppl   131.97\n",
      "| epoch   9 |  2600/ 2928 batches | lr 3.32 | ms/batch  5.86 | loss  4.90 | ppl   134.74\n",
      "| epoch   9 |  2800/ 2928 batches | lr 3.32 | ms/batch  5.88 | loss  4.83 | ppl   125.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 18.33s | valid loss  5.47 | valid ppl   237.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2928 batches | lr 3.15 | ms/batch  5.95 | loss  4.88 | ppl   131.24\n",
      "| epoch  10 |   400/ 2928 batches | lr 3.15 | ms/batch  5.93 | loss  4.90 | ppl   133.87\n",
      "| epoch  10 |   600/ 2928 batches | lr 3.15 | ms/batch  5.93 | loss  4.72 | ppl   112.36\n",
      "| epoch  10 |   800/ 2928 batches | lr 3.15 | ms/batch  5.93 | loss  4.79 | ppl   120.40\n",
      "| epoch  10 |  1000/ 2928 batches | lr 3.15 | ms/batch  5.92 | loss  4.76 | ppl   117.24\n",
      "| epoch  10 |  1200/ 2928 batches | lr 3.15 | ms/batch  5.95 | loss  4.82 | ppl   124.05\n",
      "| epoch  10 |  1400/ 2928 batches | lr 3.15 | ms/batch  5.97 | loss  4.83 | ppl   125.04\n",
      "| epoch  10 |  1600/ 2928 batches | lr 3.15 | ms/batch  5.95 | loss  4.87 | ppl   129.97\n",
      "| epoch  10 |  1800/ 2928 batches | lr 3.15 | ms/batch  6.03 | loss  4.84 | ppl   126.38\n",
      "| epoch  10 |  2000/ 2928 batches | lr 3.15 | ms/batch  5.84 | loss  4.84 | ppl   126.19\n",
      "| epoch  10 |  2200/ 2928 batches | lr 3.15 | ms/batch  5.86 | loss  4.69 | ppl   109.23\n",
      "| epoch  10 |  2400/ 2928 batches | lr 3.15 | ms/batch  5.87 | loss  4.82 | ppl   123.47\n",
      "| epoch  10 |  2600/ 2928 batches | lr 3.15 | ms/batch  5.85 | loss  4.83 | ppl   125.57\n",
      "| epoch  10 |  2800/ 2928 batches | lr 3.15 | ms/batch  6.00 | loss  4.76 | ppl   117.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 18.37s | valid loss  5.49 | valid ppl   242.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.39 | test ppl   219.63\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "#学习率调度器：它在每个训练时期（epoch）结束时更新学习率\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "bptt = 35  #序列长度\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    # train_data形状：(full_seq_len, batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        # i: 0, bptt, 2bptt, 3bptt, ..., \n",
    "        # data形状：(bptt, batch_size)\n",
    "        # targets形状：(bptt*batch_size,)\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        #output形状：(bptt, batch_size, vocab_size)\n",
    "        output = model(data)\n",
    "        #output_flat形状：(bptt*batch_size, vocab_size)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            #ppl：Perplexity\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    #不计算梯度\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs = 10\n",
    "\n",
    "#创建一个临时目录，该目录在退出 with 块时会被自动删除\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states\n",
    "\n",
    "#测试数据\n",
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nba is very popular in the bulls in the second quarter , and the second quarter , and a first play , and a first down , and a touchdown pass to the second quarter , but the second quarter , but nc state , but nc state , but nc state , but nc state , but nc state , but nc state , tech began to the ball away from the ball away from the ball away from the ball away from the ball away from the ball away from the ball away from the ball away from the ball away from\n"
     ]
    }
   ],
   "source": [
    "def lm_predict(model: nn.Module, prefix: str, max_step: int = 100) -> str:\n",
    "    #处理数据\n",
    "    tokens_index=vocab(tokenizer(prefix.strip()))\n",
    "    data=torch.tensor(tokens_index).unsqueeze(dim=1).to(device)#[seq_len, batch_size]\n",
    "    \n",
    "    model.eval()  # turn on evaluation mode\n",
    "    \n",
    "    #不计算梯度\n",
    "    with torch.no_grad():\n",
    "        for _ in torch.arange(max_step):\n",
    "            output_logits = model(data)\n",
    "            #输出索引\n",
    "            output_index = output_logits.argmax(dim=2)#[seq_len, batch_size]\n",
    "            last_index=output_index[-1:,:]\n",
    "            data = torch.cat((data,last_index),dim=0)\n",
    "            tokens=vocab.lookup_tokens(output_index.squeeze(dim=1).tolist())\n",
    "            prefix += ' '+tokens[-1]\n",
    "    return prefix\n",
    "\n",
    "prefix='nba is very popular'\n",
    "prefix=lm_predict(model,prefix)\n",
    "print(prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
